{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Add line so we don't have to reload notebooks for changes in imported modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from helpers.load_data import load_data\n",
    "from helpers.get_batch import get_batch\n",
    "from helpers.estimate_loss import estimate_loss\n",
    "from helpers.tokenizer import Tokenizer\n",
    "from models.bigram_model import BigramLanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BLOCK_SIZE = 8\n",
    "EVAL_INTERVAL = 100\n",
    "LEARNING_RATE = 1e-2\n",
    "EVAL_ITERS = 100\n",
    "MAX_ITERS = 10000\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "text: str = load_data()\n",
    "\n",
    "tokenizer = Tokenizer(text)\n",
    "data = torch.tensor(tokenizer.encode(text))\n",
    "\n",
    "# Create train and test sets\n",
    "n = int(len(text) * 0.9)\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = BigramLanguageModel(tokenizer.vocab_size).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Train loss: 2.4701, Val loss: 2.4890\n",
      "Step 100, Train loss: 2.4760, Val loss: 2.4941\n",
      "Step 200, Train loss: 2.4735, Val loss: 2.4912\n",
      "Step 300, Train loss: 2.4607, Val loss: 2.4887\n",
      "Step 400, Train loss: 2.4602, Val loss: 2.4909\n",
      "Step 500, Train loss: 2.4697, Val loss: 2.4891\n",
      "Step 600, Train loss: 2.4460, Val loss: 2.4742\n",
      "Step 700, Train loss: 2.4565, Val loss: 2.4983\n",
      "Step 800, Train loss: 2.4672, Val loss: 2.4757\n",
      "Step 900, Train loss: 2.4517, Val loss: 2.4885\n",
      "Step 1000, Train loss: 2.4528, Val loss: 2.4912\n",
      "Step 1100, Train loss: 2.4519, Val loss: 2.4841\n",
      "Step 1200, Train loss: 2.4577, Val loss: 2.4762\n",
      "Step 1300, Train loss: 2.4523, Val loss: 2.4853\n",
      "Step 1400, Train loss: 2.4463, Val loss: 2.4940\n",
      "Step 1500, Train loss: 2.4494, Val loss: 2.4841\n",
      "Step 1600, Train loss: 2.4731, Val loss: 2.4828\n",
      "Step 1700, Train loss: 2.4469, Val loss: 2.4820\n",
      "Step 1800, Train loss: 2.4537, Val loss: 2.5005\n",
      "Step 1900, Train loss: 2.4587, Val loss: 2.4823\n",
      "Step 2000, Train loss: 2.4630, Val loss: 2.4896\n",
      "Step 2100, Train loss: 2.4762, Val loss: 2.5035\n",
      "Step 2200, Train loss: 2.4635, Val loss: 2.4894\n",
      "Step 2300, Train loss: 2.4677, Val loss: 2.5014\n",
      "Step 2400, Train loss: 2.4546, Val loss: 2.4868\n",
      "Step 2500, Train loss: 2.4582, Val loss: 2.4884\n",
      "Step 2600, Train loss: 2.4496, Val loss: 2.4872\n",
      "Step 2700, Train loss: 2.4585, Val loss: 2.4863\n",
      "Step 2800, Train loss: 2.4536, Val loss: 2.4810\n",
      "Step 2900, Train loss: 2.4450, Val loss: 2.4881\n",
      "Step 3000, Train loss: 2.4636, Val loss: 2.4988\n",
      "Step 3100, Train loss: 2.4488, Val loss: 2.4903\n",
      "Step 3200, Train loss: 2.4597, Val loss: 2.4811\n",
      "Step 3300, Train loss: 2.4469, Val loss: 2.4874\n",
      "Step 3400, Train loss: 2.4590, Val loss: 2.4773\n",
      "Step 3500, Train loss: 2.4526, Val loss: 2.4756\n",
      "Step 3600, Train loss: 2.4567, Val loss: 2.4982\n",
      "Step 3700, Train loss: 2.4577, Val loss: 2.5028\n",
      "Step 3800, Train loss: 2.4510, Val loss: 2.5059\n",
      "Step 3900, Train loss: 2.4578, Val loss: 2.4912\n",
      "Step 4000, Train loss: 2.4498, Val loss: 2.4868\n",
      "Step 4100, Train loss: 2.4480, Val loss: 2.4850\n",
      "Step 4200, Train loss: 2.4442, Val loss: 2.4846\n",
      "Step 4300, Train loss: 2.4579, Val loss: 2.4763\n",
      "Step 4400, Train loss: 2.4510, Val loss: 2.4764\n",
      "Step 4500, Train loss: 2.4460, Val loss: 2.4830\n",
      "Step 4600, Train loss: 2.4484, Val loss: 2.4838\n",
      "Step 4700, Train loss: 2.4619, Val loss: 2.4814\n",
      "Step 4800, Train loss: 2.4523, Val loss: 2.4937\n",
      "Step 4900, Train loss: 2.4455, Val loss: 2.4809\n",
      "Step 5000, Train loss: 2.4575, Val loss: 2.4855\n",
      "Step 5100, Train loss: 2.4511, Val loss: 2.4723\n",
      "Step 5200, Train loss: 2.4457, Val loss: 2.5034\n",
      "Step 5300, Train loss: 2.4498, Val loss: 2.4905\n",
      "Step 5400, Train loss: 2.4342, Val loss: 2.4891\n",
      "Step 5500, Train loss: 2.4638, Val loss: 2.4821\n",
      "Step 5600, Train loss: 2.4562, Val loss: 2.4737\n",
      "Step 5700, Train loss: 2.4581, Val loss: 2.4988\n",
      "Step 5800, Train loss: 2.4474, Val loss: 2.4761\n",
      "Step 5900, Train loss: 2.4615, Val loss: 2.4889\n",
      "Step 6000, Train loss: 2.4733, Val loss: 2.4844\n",
      "Step 6100, Train loss: 2.4482, Val loss: 2.4807\n",
      "Step 6200, Train loss: 2.4472, Val loss: 2.4990\n",
      "Step 6300, Train loss: 2.4499, Val loss: 2.4821\n",
      "Step 6400, Train loss: 2.4552, Val loss: 2.4870\n",
      "Step 6500, Train loss: 2.4535, Val loss: 2.4928\n",
      "Step 6600, Train loss: 2.4565, Val loss: 2.4724\n",
      "Step 6700, Train loss: 2.4614, Val loss: 2.4860\n",
      "Step 6800, Train loss: 2.4573, Val loss: 2.4863\n",
      "Step 6900, Train loss: 2.4559, Val loss: 2.4822\n",
      "Step 7000, Train loss: 2.4519, Val loss: 2.4953\n",
      "Step 7100, Train loss: 2.4519, Val loss: 2.5009\n",
      "Step 7200, Train loss: 2.4551, Val loss: 2.4980\n",
      "Step 7300, Train loss: 2.4550, Val loss: 2.4816\n",
      "Step 7400, Train loss: 2.4526, Val loss: 2.4878\n",
      "Step 7500, Train loss: 2.4565, Val loss: 2.4691\n",
      "Step 7600, Train loss: 2.4469, Val loss: 2.4771\n",
      "Step 7700, Train loss: 2.4610, Val loss: 2.4955\n",
      "Step 7800, Train loss: 2.4508, Val loss: 2.4707\n",
      "Step 7900, Train loss: 2.4628, Val loss: 2.4795\n",
      "Step 8000, Train loss: 2.4603, Val loss: 2.4798\n",
      "Step 8100, Train loss: 2.4734, Val loss: 2.4908\n",
      "Step 8200, Train loss: 2.4539, Val loss: 2.4674\n",
      "Step 8300, Train loss: 2.4554, Val loss: 2.4890\n",
      "Step 8400, Train loss: 2.4500, Val loss: 2.4978\n",
      "Step 8500, Train loss: 2.4462, Val loss: 2.4825\n",
      "Step 8600, Train loss: 2.4659, Val loss: 2.4797\n",
      "Step 8700, Train loss: 2.4551, Val loss: 2.4826\n",
      "Step 8800, Train loss: 2.4562, Val loss: 2.4783\n",
      "Step 8900, Train loss: 2.4602, Val loss: 2.4831\n",
      "Step 9000, Train loss: 2.4592, Val loss: 2.4861\n",
      "Step 9100, Train loss: 2.4569, Val loss: 2.4971\n",
      "Step 9200, Train loss: 2.4469, Val loss: 2.4813\n",
      "Step 9300, Train loss: 2.4528, Val loss: 2.4813\n",
      "Step 9400, Train loss: 2.4582, Val loss: 2.4919\n",
      "Step 9500, Train loss: 2.4538, Val loss: 2.4874\n",
      "Step 9600, Train loss: 2.4501, Val loss: 2.4757\n",
      "Step 9700, Train loss: 2.4594, Val loss: 2.4965\n",
      "Step 9800, Train loss: 2.4499, Val loss: 2.4923\n",
      "Step 9900, Train loss: 2.4625, Val loss: 2.4840\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(bigram_model.parameters(), lr=1e-3)\n",
    "\n",
    "for iter in range(MAX_ITERS):\n",
    "    \n",
    "    if iter % EVAL_INTERVAL == 0:\n",
    "        train_loss, val_loss = estimate_loss(\n",
    "            model=bigram_model, \n",
    "            train_data=train_data,\n",
    "            valid_data=test_data,\n",
    "            block_size=BLOCK_SIZE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            eval_iters=EVAL_ITERS,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        print(f\"Step {iter}, Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch(\n",
    "        train_data,\n",
    "        BLOCK_SIZE, \n",
    "        BATCH_SIZE,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    logits, loss = bigram_model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DYo spour he fo t ots iell f,\n",
      "Tot,--h n d merit ira my?\n",
      "Anso 's?\n",
      "Whagethe nt h blfore lld rus an h O:\n",
      "ail Cortis l tingu He esedass thil the mbel's ckineannor 's hmbo moutoorseis tex--\n",
      "S:\n",
      "HEThouth tomerover hist'dond an;\n",
      "NTh.\n",
      "'d opus d ind tend ngonor ma mse bs!\n",
      "Thid to nd wo s thatom.\n",
      "\n",
      "\n",
      "\n",
      "KIO:\n",
      "KETald yobly tild:\n",
      "AMineromu l s e\n",
      "te tof we ild.\n",
      "QULIUS:\n",
      "TI busporknes\n",
      "Whe halour akes mand!\n",
      "ILABY miss sst obes cathid fathay, asen wavitlm d stha:\n",
      "CLI loche!\n",
      "FO:\n",
      "\n",
      "G ame.\n",
      "\n",
      "ADUpie od; he hy banwody\n",
      "GBO,\n",
      "\n",
      "W\n"
     ]
    }
   ],
   "source": [
    "start_token = torch.zeros((1, 1)).long().to(DEVICE)\n",
    "sequence = bigram_model.generate(start_token, max_len=500)[0].tolist()\n",
    "print(tokenizer.decode(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeris((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0) # (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tril(torch.ones(3,3))\n",
    "weights = weights / torch.sum(weights, 1, keepdim=True)\n",
    "xbow2 = weights @ x # (B, T, T) @ (B, T, C) -> (B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "B,T,C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, 1)\n",
    "\n",
    "v = value(x) # (B, T, C)\n",
    "out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "\n",
    "out = wei @ x # (T, T) @ (B, T, C) -> (B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wei = q @ k.transpose(-2, -1) / np.sqrt(head_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
