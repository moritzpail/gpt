{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuN7mEbsENrt"
      },
      "source": [
        "# Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4F3riXUENrv"
      },
      "source": [
        "You can find the full code for this notebook [here](https://github.com/moritzpail/gpt). In addition to the config that Karpathy uses in his [tutorial](https://www.youtube.com/watch?v=kCc8FmEb1nY), this notebook implements a different model hidden dimension, optimizer, and uses GeLU instead of ReLU in ffwd network of the model. In particular, we\n",
        "- Use a hidden dimension size of 128 instead of 384 to keep the training more manageable with the available resources (free GPU on Google Colab). We also use smaller values for n_layers, n_heads, and block_size.\n",
        "- Use GeLU instead of ReLU in the FFWD network of the GPT as I read that this might give improvement.\n",
        "- Use the Lion optimizer as I read some [evidence](https://github.com/lucidrains/lion-pytorch) that this might also lead to efficiency gains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjDblG48ENrv"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/moritzpail/gpt.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "q4gO2hZJEaFG",
        "outputId": "3ed75f1b-0341-4a96-b960-955b418f89ef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gpt'...\n",
            "remote: Enumerating objects: 42, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 42 (delta 11), reused 37 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (42/42), 450.89 KiB | 13.26 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pFdaomhdENrw"
      },
      "outputs": [],
      "source": [
        "# Add line so we don't have to reload notebooks for changes in imported modules\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "iWAZgvqeEVxt",
        "outputId": "8e4bbe77-4e56-4fd9-b81a-7ab01dda2931"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lion_pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "T-YH9DGsFJqh",
        "outputId": "ce42ec61-24aa-439e-9e7c-ccbc35359e99"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lion_pytorch\n",
            "  Downloading lion_pytorch-0.2.2-py3-none-any.whl.metadata (618 bytes)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from lion_pytorch) (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->lion_pytorch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->lion_pytorch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->lion_pytorch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->lion_pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->lion_pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->lion_pytorch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.6->lion_pytorch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.6->lion_pytorch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.6->lion_pytorch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.6->lion_pytorch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.6->lion_pytorch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.6->lion_pytorch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.6->lion_pytorch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.6->lion_pytorch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.6->lion_pytorch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.6->lion_pytorch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.6->lion_pytorch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->lion_pytorch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6->lion_pytorch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->lion_pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->lion_pytorch) (1.3.0)\n",
            "Downloading lion_pytorch-0.2.2-py3-none-any.whl (5.4 kB)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, lion_pytorch\n",
            "Successfully installed lion_pytorch-0.2.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9Td81bgrENrw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from lion_pytorch import Lion\n",
        "\n",
        "from helpers.load_data import load_data\n",
        "from helpers.get_batch import get_batch\n",
        "from helpers.estimate_loss import estimate_loss\n",
        "from helpers.tokenizer import Tokenizer\n",
        "from models.gpt import GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4AgKt7e8ENrx",
        "outputId": "6196e2a9-c49e-49e1-b269-44cb6359cfb8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7bda4efe68f0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "torch.manual_seed(13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INRMViVBENrx"
      },
      "source": [
        "# Globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "up5HEHjLENrx"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "BLOCK_SIZE = 128\n",
        "EVAL_INTERVAL = 100\n",
        "LEARNING_RATE = 3e-4\n",
        "EVAL_ITERS = 500\n",
        "MAX_ITERS = 5000\n",
        "N_EMBED = 64\n",
        "N_HEADS = 4\n",
        "N_LAYERS = 4\n",
        "DROPOUT_RATE = 0.2\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4nINHi4ENrx"
      },
      "source": [
        "# Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1d-F_nUdENrx"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "text: str = load_data()\n",
        "\n",
        "tokenizer = Tokenizer(text)\n",
        "data = torch.tensor(tokenizer.encode(text))\n",
        "\n",
        "# Create train and test sets\n",
        "n = int(len(text) * 0.9)\n",
        "train_data = data[:n]\n",
        "test_data = data[n:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wZ0NRs_ENrx"
      },
      "source": [
        "# GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XBTIBepqENrx"
      },
      "outputs": [],
      "source": [
        "gpt = GPT(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    n_embed_size=N_EMBED,\n",
        "    block_size=BLOCK_SIZE,\n",
        "    device=DEVICE,\n",
        "    n_heads=N_HEADS,\n",
        "    n_layers=N_LAYERS,\n",
        "    dropout_rate=DROPOUT_RATE\n",
        ").to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69XCU-f_ENry"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "aV2HkuY_ENry",
        "outputId": "9e68be59-61f2-4883-f8c5-ada43beaf341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, Train loss: 4.3478, Val loss: 4.3510\n",
            "Step 100, Train loss: 2.9397, Val loss: 2.9733\n",
            "Step 200, Train loss: 2.6640, Val loss: 2.6738\n",
            "Step 300, Train loss: 2.5418, Val loss: 2.5442\n",
            "Step 400, Train loss: 2.4791, Val loss: 2.4823\n",
            "Step 500, Train loss: 2.4309, Val loss: 2.4435\n",
            "Step 600, Train loss: 2.3884, Val loss: 2.4036\n",
            "Step 700, Train loss: 2.3458, Val loss: 2.3656\n",
            "Step 800, Train loss: 2.2910, Val loss: 2.3174\n",
            "Step 900, Train loss: 2.2429, Val loss: 2.2815\n",
            "Step 1000, Train loss: 2.1979, Val loss: 2.2414\n",
            "Step 1100, Train loss: 2.1585, Val loss: 2.2091\n",
            "Step 1200, Train loss: 2.1202, Val loss: 2.1753\n",
            "Step 1300, Train loss: 2.0739, Val loss: 2.1384\n",
            "Step 1400, Train loss: 2.0297, Val loss: 2.0966\n",
            "Step 1500, Train loss: 1.9927, Val loss: 2.0689\n",
            "Step 1600, Train loss: 1.9575, Val loss: 2.0347\n",
            "Step 1700, Train loss: 1.9176, Val loss: 2.0069\n",
            "Step 1800, Train loss: 1.8895, Val loss: 1.9879\n",
            "Step 1900, Train loss: 1.8600, Val loss: 1.9754\n",
            "Step 2000, Train loss: 1.8327, Val loss: 1.9567\n",
            "Step 2100, Train loss: 1.8116, Val loss: 1.9465\n",
            "Step 2200, Train loss: 1.7851, Val loss: 1.9202\n",
            "Step 2300, Train loss: 1.7671, Val loss: 1.9096\n",
            "Step 2400, Train loss: 1.7505, Val loss: 1.8953\n",
            "Step 2500, Train loss: 1.7325, Val loss: 1.8829\n",
            "Step 2600, Train loss: 1.7181, Val loss: 1.8655\n",
            "Step 2700, Train loss: 1.7041, Val loss: 1.8613\n",
            "Step 2800, Train loss: 1.6911, Val loss: 1.8488\n",
            "Step 2900, Train loss: 1.6740, Val loss: 1.8388\n",
            "Step 3000, Train loss: 1.6634, Val loss: 1.8289\n",
            "Step 3100, Train loss: 1.6524, Val loss: 1.8239\n",
            "Step 3200, Train loss: 1.6449, Val loss: 1.8136\n",
            "Step 3300, Train loss: 1.6334, Val loss: 1.8031\n",
            "Step 3400, Train loss: 1.6245, Val loss: 1.8028\n",
            "Step 3500, Train loss: 1.6166, Val loss: 1.7905\n",
            "Step 3600, Train loss: 1.6046, Val loss: 1.7831\n",
            "Step 3700, Train loss: 1.6047, Val loss: 1.7888\n",
            "Step 3800, Train loss: 1.5905, Val loss: 1.7817\n",
            "Step 3900, Train loss: 1.5852, Val loss: 1.7684\n",
            "Step 4000, Train loss: 1.5813, Val loss: 1.7593\n",
            "Step 4100, Train loss: 1.5726, Val loss: 1.7564\n",
            "Step 4200, Train loss: 1.5649, Val loss: 1.7536\n",
            "Step 4300, Train loss: 1.5590, Val loss: 1.7453\n",
            "Step 4400, Train loss: 1.5552, Val loss: 1.7440\n",
            "Step 4500, Train loss: 1.5489, Val loss: 1.7354\n",
            "Step 4600, Train loss: 1.5440, Val loss: 1.7364\n",
            "Step 4700, Train loss: 1.5445, Val loss: 1.7344\n",
            "Step 4800, Train loss: 1.5356, Val loss: 1.7271\n",
            "Step 4900, Train loss: 1.5328, Val loss: 1.7253\n"
          ]
        }
      ],
      "source": [
        "# optimizer = torch.optim.AdamW(gpt.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = Lion(gpt.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "for iter in range(MAX_ITERS):\n",
        "\n",
        "    if iter % EVAL_INTERVAL == 0:\n",
        "        train_loss, val_loss = estimate_loss(\n",
        "            model=gpt,\n",
        "            train_data=train_data,\n",
        "            valid_data=test_data,\n",
        "            block_size=BLOCK_SIZE,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            eval_iters=EVAL_ITERS,\n",
        "            device=DEVICE\n",
        "        )\n",
        "        print(f\"Step {iter}, Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch(\n",
        "        train_data,\n",
        "        BLOCK_SIZE,\n",
        "        BATCH_SIZE,\n",
        "        device=DEVICE\n",
        "    )\n",
        "\n",
        "    logits, loss = gpt(xb, yb)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "X0K3OFnGENry",
        "outputId": "1f8c1cb3-e349-4768-a0ba-de5fbe0401aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EXETER:\n",
            "You hunk good.\n",
            "Your 'sount her by his duke\n",
            "With feart, not homing rofe, make!\n",
            "\n",
            "LEONTES:\n",
            "And choose is est's make it that other. Slay's far? Thing.\n",
            "\n",
            "THARDY I sreak not by: Pecarity her; in ruder\n",
            "of madamianeton, vorery comesterly be eartain with\n",
            "of his grands fair sold away forth.\n",
            "\n",
            "LUCIO:\n",
            "Bray, if I that sirment prince sented; I.\n",
            "I would, his greath, an hopes toward,-sish,\n",
            "Roim must a has had recure ye himself owerence\n",
            "Whert that oun such revard more than which forten,-\n",
            "Thy works; all not\n"
          ]
        }
      ],
      "source": [
        "start_token = torch.zeros((1, 1)).long().to(DEVICE)\n",
        "sequence = gpt.generate(start_token, max_len=500, block_size=BLOCK_SIZE)[0].tolist()\n",
        "print(tokenizer.decode(sequence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1WmRg1ZENry"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}